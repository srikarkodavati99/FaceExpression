{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OrdinalEncoder"
      ],
      "metadata": {
        "id": "rP6qA82hazkS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "HQ1Ra2zd9CU6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nALkuO-G9DWF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "  \"\"\"\"Generate data for time series creating windows of given n.\"\"\"\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "metadata": {
        "id": "1Sl3L4yBa2hH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacing_window(data,win_space):\n",
        "  \"\"\"\"creating windows with specific frame.\"\"\"\n",
        "  data=data[data.index%win_space == 0]\n",
        "  return data"
      ],
      "metadata": {
        "id": "NRjjSEY-q1dv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_complete=pd.read_csv('/content/drive/MyDrive/data_gan_final.csv')\n",
        "ord_enc = OrdinalEncoder()\n",
        "dataset_complete[\"Word_code\"] = ord_enc.fit_transform(dataset_complete[[\"Word\"]])\n",
        "dataset_complete=dataset_complete.drop(['Word'], axis=1)\n",
        "\n",
        "x=max(dataset_complete['ID'])\n",
        "y=min(dataset_complete['ID'])\n",
        "print(x)\n",
        "print(y)\n",
        "TRAIN=np.array([])\n",
        "TEST=np.array([])\n",
        "for i in range (y,x+1):\n",
        "  dataset=dataset_complete[dataset_complete['ID']== i]\n",
        "  dataset_temp=dataset[dataset.columns.drop(list(dataset.filter(regex='_r$')))]\n",
        "  dataset1=dataset.drop(['ID', 'File'], axis=1)\n",
        "  \n",
        "  dataset_out=dataset_temp.drop(['ID', 'File'], axis=1)\n",
        "  \n",
        "  values1 = dataset1.values\n",
        "  values_out=dataset_out.values\n",
        "  n_hours = 30\n",
        "  n_features=28\n",
        "  n_out_features = 11\n",
        "  # # convert frame as time series data\n",
        "  reframed1 = series_to_supervised(values1, n_hours, 1)\n",
        "  reframed1=spacing_window(reframed1,10)\n",
        "  values1 = reframed1.values\n",
        "  reframed_out = series_to_supervised(values_out, n_hours, 1)\n",
        "  reframed_out=spacing_window(reframed_out,10)\n",
        "  values_out = reframed_out.values\n",
        "  # test and train split\n",
        "  train = values1\n",
        "  test=values_out\n",
        "  n_obs = n_hours * n_features\n",
        "  \n",
        "  train = train.reshape((train.shape[0], n_hours, n_features))\n",
        "  test = test.reshape((test.shape[0], n_hours, n_out_features))\n",
        "  print(train.shape, test.shape)\n",
        "  if len(TRAIN)==0:\n",
        "    TRAIN=train\n",
        "    TEST=test\n",
        "    \n",
        "  else:\n",
        "    TRAIN=np.concatenate((TRAIN, train))\n",
        "    TEST=np.concatenate((TEST, test))\n",
        "    \n",
        "  print('@@@@@@@@@@@@@@@@@@@@')\n",
        "  print(TRAIN.shape, TEST.shape)\n"
      ],
      "metadata": {
        "id": "0x6tJZuUq2qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=TRAIN"
      ],
      "metadata": {
        "id": "ARx6EuMdtstg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 30\n",
        "n_seq = 17\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "SGtw0OFQrZNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_windows=len(data)"
      ],
      "metadata": {
        "id": "DJzl5upWjOBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "qAR-KqtMi3wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_series = (tf.data.Dataset\n",
        "               .from_tensor_slices(data)\n",
        "              #  .shuffle(buffer_size=n_windows)\n",
        "               .batch(128))\n",
        "real_series_iter = iter(real_series.repeat())"
      ],
      "metadata": {
        "id": "rSq9Gb2WfsVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_random_data():\n",
        "    while True:\n",
        "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))"
      ],
      "metadata": {
        "id": "O0einD57xYZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=TEST\n",
        "data1.shape"
      ],
      "metadata": {
        "id": "bZLSCu6NxQdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_series1 = (tf.data.Dataset\n",
        "               .from_tensor_slices(data1)\n",
        "              #  .shuffle(buffer_size=n_windows)\n",
        "               .batch(128))\n",
        "real_series1_iter = iter(real_series1.repeat())"
      ],
      "metadata": {
        "id": "s1Vvat52xLh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_series = iter(tf.data.Dataset\n",
        "                     .from_generator(make_random_data, output_types=tf.float32)\n",
        "                     .batch(batch_size)\n",
        "                     .repeat())"
      ],
      "metadata": {
        "id": "uwnqNkojx3cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pickle import dump\n",
        "# dump(ord_enc, open('ordinal.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "LtTDK5AYmZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pickle import load\n",
        "# ord_enc = load(open('ordinal.pkl', 'rb'))\n",
        "# xx=ord_enc.transform([['my']])\n",
        "# print(xx)"
      ],
      "metadata": {
        "id": "XFuySWVmnQkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 56\n",
        "num_layers = 3"
      ],
      "metadata": {
        "id": "I1U0-ZEyytJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = Input(shape=[seq_len, 28], name='RealData')\n",
        "Z = Input(shape=[seq_len, 28], name='RandomData')"
      ],
      "metadata": {
        "id": "HblC7N2wzACI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_rnn(n_layers, hidden_units, output_units, name):\n",
        "    \"\"\" GRU layers \"\"\"\n",
        "    return Sequential([GRU(units=hidden_units,\n",
        "                           return_sequences=True,\n",
        "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
        "                      [Dense(units=output_units,\n",
        "                             activation='sigmoid',\n",
        "                             name='OUT')], name=name)"
      ],
      "metadata": {
        "id": "7yZb6uJjzHFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = make_rnn(n_layers=3, \n",
        "                    hidden_units=hidden_dim, \n",
        "                    output_units=hidden_dim, \n",
        "                    name='Embedder')\n",
        "recovery = make_rnn(n_layers=3, \n",
        "                    hidden_units=hidden_dim, \n",
        "                    output_units=n_seq, \n",
        "                    name='Recovery')"
      ],
      "metadata": {
        "id": "VcKB7Kpe3mEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_rnn(n_layers=3, \n",
        "                     hidden_units=hidden_dim, \n",
        "                     output_units=hidden_dim, \n",
        "                     name='Generator')\n",
        "discriminator = make_rnn(n_layers=3, \n",
        "                         hidden_units=hidden_dim, \n",
        "                         output_units=1, \n",
        "                         name='Discriminator')\n",
        "supervisor = make_rnn(n_layers=2, \n",
        "                      hidden_units=hidden_dim, \n",
        "                      output_units=hidden_dim, \n",
        "                      name='Supervisor')"
      ],
      "metadata": {
        "id": "XU0eDXFG5n4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = 10000\n",
        "gamma = 1"
      ],
      "metadata": {
        "id": "2h7Vm95T6avG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = MeanSquaredError()\n",
        "bce = BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "lNr8esFY7gCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H = embedder(X)\n",
        "X_tilde = recovery(H)\n",
        "\n",
        "autoencoder = Model(inputs=X,\n",
        "                    outputs=X_tilde,\n",
        "                    name='Autoencoder')"
      ],
      "metadata": {
        "id": "SF7Y7t719V4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "km0A56vQ9XKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_optimizer = Adam()"
      ],
      "metadata": {
        "id": "h4We5kF0X2fD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_autoencoder_init(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        x_tilde = autoencoder(x)\n",
        "        x.shape\n",
        "        embedding_loss_t0 = mse(x[:,:,:17], x_tilde)\n",
        "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
        "\n",
        "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
        "    gradients = tape.gradient(e_loss_0, var_list)\n",
        "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return tf.sqrt(embedding_loss_t0)"
      ],
      "metadata": {
        "id": "f4LMgc9AX7Rl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in tqdm(range(train_steps)):\n",
        "    X_ = next(real_series_iter)\n",
        "    next(real_series1_iter)\n",
        "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
        "    "
      ],
      "metadata": {
        "id": "9DojnEAYYDDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supervisor_optimizer = Adam()"
      ],
      "metadata": {
        "id": "JL_ZXeZGYRyv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_supervisor(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])\n",
        "\n",
        "    var_list = supervisor.trainable_variables\n",
        "    gradients = tape.gradient(g_loss_s, var_list)\n",
        "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return g_loss_s"
      ],
      "metadata": {
        "id": "BE4rT109YamT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in tqdm(range(train_steps)):\n",
        "    X_ = next(real_series_iter)\n",
        "    next(real_series1_iter)\n",
        "    step_g_loss_s = train_supervisor(X_)\n",
        "    "
      ],
      "metadata": {
        "id": "d-VCR2TnYn-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E_hat = generator(Z)\n",
        "H_hat = supervisor(E_hat)\n",
        "Y_fake = discriminator(H_hat)\n",
        "\n",
        "adversarial_supervised = Model(inputs=Z,\n",
        "                               outputs=Y_fake,\n",
        "                               name='AdversarialNetSupervised')"
      ],
      "metadata": {
        "id": "PbLmrQ0gY7lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial_supervised.summary()"
      ],
      "metadata": {
        "id": "we9iaF8mY_bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_fake_e = discriminator(E_hat)\n",
        "\n",
        "adversarial_emb = Model(inputs=Z,\n",
        "                    outputs=Y_fake_e,\n",
        "                    name='AdversarialNet')"
      ],
      "metadata": {
        "id": "_tzyhpDdZS7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adversarial_emb.summary()"
      ],
      "metadata": {
        "id": "V0AFpf28ZXFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_hat = recovery(H_hat)\n",
        "synthetic_data = Model(inputs=Z,\n",
        "                       outputs=X_hat,\n",
        "                       name='SyntheticData')"
      ],
      "metadata": {
        "id": "S2bFntTNZnF1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_hat = recovery(H_hat)\n",
        "synthetic_data = Model(inputs=Z,\n",
        "                       outputs=X_hat,\n",
        "                       name='SyntheticData')"
      ],
      "metadata": {
        "id": "wwH_cKDPcf0i"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data.summary()"
      ],
      "metadata": {
        "id": "aw5j5EticjS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_generator_moment_loss(y_true, y_pred):\n",
        "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
        "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
        "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
        "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
        "    return g_loss_mean + g_loss_var"
      ],
      "metadata": {
        "id": "_syuGkUhc8eE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_real = discriminator(H)\n",
        "discriminator_model = Model(inputs=X,\n",
        "                            outputs=Y_real,\n",
        "                            name='DiscriminatorReal')"
      ],
      "metadata": {
        "id": "CA4_o3tic9nA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model.summary()"
      ],
      "metadata": {
        "id": "x1mBlknWdDvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = Adam()\n",
        "discriminator_optimizer = Adam()\n",
        "embedding_optimizer = Adam()"
      ],
      "metadata": {
        "id": "XoS5xrowdMNW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_generator(x, z):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_fake = adversarial_supervised(z)\n",
        "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
        "                                          y_pred=y_fake)\n",
        "\n",
        "        y_fake_e = adversarial_emb(z)\n",
        "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
        "                                            y_pred=y_fake_e)\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
        "\n",
        "        x_hat = synthetic_data(z)\n",
        "        x=tf.cast(x,tf.float32)\n",
        "\n",
        "        generator_moment_loss = get_generator_moment_loss(x[:,:,:17], x_hat)\n",
        "\n",
        "        generator_loss = (generator_loss_unsupervised +\n",
        "                          generator_loss_unsupervised_e +\n",
        "                          100 * tf.sqrt(generator_loss_supervised) +\n",
        "                          100 * generator_moment_loss)\n",
        "\n",
        "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
        "    gradients = tape.gradient(generator_loss, var_list)\n",
        "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss"
      ],
      "metadata": {
        "id": "Ulrz6_4OdQs4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_embedder(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        h = embedder(x)\n",
        "        h_hat_supervised = supervisor(h)\n",
        "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
        "\n",
        "        x_tilde = autoencoder(x)\n",
        "        embedding_loss_t0 = mse(x[:,:,:17], x_tilde)\n",
        "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
        "\n",
        "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
        "    gradients = tape.gradient(e_loss, var_list)\n",
        "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return tf.sqrt(embedding_loss_t0)"
      ],
      "metadata": {
        "id": "cvr15Vf3fv9F"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def get_discriminator_loss(x, z):\n",
        "    y_real = discriminator_model(x)\n",
        "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
        "                                  y_pred=y_real)\n",
        "\n",
        "    y_fake = adversarial_supervised(z)\n",
        "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
        "                                  y_pred=y_fake)\n",
        "\n",
        "    y_fake_e = adversarial_emb(z)\n",
        "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
        "                                    y_pred=y_fake_e)\n",
        "    return (discriminator_loss_real +\n",
        "            discriminator_loss_fake +\n",
        "            gamma * discriminator_loss_fake_e)"
      ],
      "metadata": {
        "id": "gYjlgHdLfxO7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_discriminator(x, z):\n",
        "    with tf.GradientTape() as tape:\n",
        "        discriminator_loss = get_discriminator_loss(x, z)\n",
        "\n",
        "    var_list = discriminator.trainable_variables\n",
        "    gradients = tape.gradient(discriminator_loss, var_list)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
        "    return discriminator_loss"
      ],
      "metadata": {
        "id": "Jed9bSMyf1In"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
        "count=0\n",
        "for step in range(train_steps):\n",
        "    # Train generator (twice as often as discriminator)\n",
        "    for kk in range(2):\n",
        "        X_ = next(real_series_iter)\n",
        "        # Z_ = next(random_series)\n",
        "        x=next(real_series1_iter)\n",
        "        if x.shape[0] != 128:\n",
        "          print(x.shape)\n",
        "          X_ = next(real_series_iter)\n",
        "          # Z_ = next(random_series)\n",
        "          x=next(real_series1_iter)\n",
        "          count=count+1\n",
        "        if count==1:\n",
        "          print(x.shape)\n",
        "          count=0\n",
        "        y=next(random_series)\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        Z_=tf.concat([x, y], 2)\n",
        "        # Train generator\n",
        "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
        "        # Train embedder\n",
        "        step_e_loss_t0 = train_embedder(X_)\n",
        "\n",
        "    X_ = next(real_series_iter)\n",
        "    x=next(real_series1_iter)\n",
        "    if x.shape[0] != 128:\n",
        "      print(x.shape)\n",
        "      X_ = next(real_series_iter)\n",
        "      # Z_ = next(random_series)\n",
        "      x=next(real_series1_iter)\n",
        "      count=count+1\n",
        "    if count==1:\n",
        "      print(x.shape)\n",
        "      count=0\n",
        "    y=next(random_series)\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    Z_=tf.concat([x, y], 2)\n",
        "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
        "    if step_d_loss > 0.15:\n",
        "        step_d_loss = train_discriminator(X_, Z_)\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
        "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "mk1lyF4Mf5XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data.save('synthetic_data')"
      ],
      "metadata": {
        "id": "QIyS_fDIf_H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data = []\n",
        "for i in range(int(n_windows / batch_size)):\n",
        "    x=next(real_series1_iter)\n",
        "    y=next(random_series)\n",
        "    x = tf.cast(x, tf.float32)\n",
        "    Z_=tf.concat([x, y], 2)\n",
        "    d = synthetic_data(Z_)\n",
        "    generated_data.append(d)"
      ],
      "metadata": {
        "id": "x1lOzI31gJPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(generated_data)"
      ],
      "metadata": {
        "id": "s5z9SvxZgNl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_data = np.array(np.vstack(generated_data))\n",
        "generated_data.shape"
      ],
      "metadata": {
        "id": "KEpQL_Q_gRS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJeREIVqMN0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}